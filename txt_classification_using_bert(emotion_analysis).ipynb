{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3yXqZB0_BJoM",
        "outputId": "e6201e4c-3d50-4478-d096-e7a51155a4f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-16.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Collecting requests (from transformers)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 16.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 16.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 multiprocess-0.70.16 pyarrow-16.1.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow",
                  "requests"
                ]
              },
              "id": "a71f195c835e4b329eb717ae1a1b3949"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqHoYuJ7CQ-y",
        "outputId": "2b8752a4-db80-4090-f7b8-f5c2c1aa0974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.0+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.31.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "pKP9GYteBK0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"emotion\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCItBK5lBvS7",
        "outputId": "5f3aeda3-fc8e-49ad-f9b9-ff39d5d5ef70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 16000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 2000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the unique labels in the dataset\n",
        "unique_labels = dataset['train'].features['label'].names\n",
        "num_labels = len(unique_labels)\n",
        "\n",
        "print(f\"Unique labels: {unique_labels}\")\n",
        "print(f\"Number of labels: {num_labels}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdG4ZPj5U7O9",
        "outputId": "cccd7e8e-a6a7-47ce-9cdf-29a943f7365c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels: ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
            "Number of labels: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_dataset = tokenized_datasets['train']\n",
        "test_dataset = tokenized_datasets['test']\n"
      ],
      "metadata": {
        "id": "NKl7GblQBK3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"emotion\")\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Use a subset of the training dataset\n",
        "train_dataset = tokenized_datasets['train'].shuffle(seed=42).select(range(3000))  # Using only 500 samples\n",
        "test_dataset = tokenized_datasets['test']\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,  # Reduced epochs\n",
        "    weight_decay=0.01,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        ")\n",
        "\n",
        "# Define compute metrics function\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "FJvuiz-4BV37",
        "outputId": "15678611-3b3e-4d9c-ee4e-a27b4840caca"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='121' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [121/940 00:49 < 05:37, 2.43 it/s, Epoch 0.64/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='940' max='940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [940/940 07:55, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.698293</td>\n",
              "      <td>0.751000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.355336</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.675100</td>\n",
              "      <td>0.301202</td>\n",
              "      <td>0.899000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.675100</td>\n",
              "      <td>0.302776</td>\n",
              "      <td>0.903500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.675100</td>\n",
              "      <td>0.317325</td>\n",
              "      <td>0.907500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=940, training_loss=0.4094144293602477, metrics={'train_runtime': 476.2572, 'train_samples_per_second': 31.496, 'train_steps_per_second': 1.974, 'total_flos': 3946807572480000.0, 'train_loss': 0.4094144293602477, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgNuqNJXBHqd",
        "outputId": "960b70d6-7d7b-4775-9776-63d2159d173b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am happy today: happy\n",
            "The weather is gloomy: sad\n",
            "why are you irritating me: angry\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Define the mapping from label IDs to emotion names\n",
        "id2label = {\n",
        "    0: \"sad\",\n",
        "    1: \"happy\",\n",
        "    2: \"love\",\n",
        "    3: \"angry\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# Update the model's config with this mapping\n",
        "model.config.id2label = id2label\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_emotions(text):\n",
        "    sentences = text.split('.')\n",
        "    results = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip():  # Ignore empty sentences\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            predicted_class_id = logits.argmax().item()\n",
        "            predicted_emotion = model.config.id2label[predicted_class_id]\n",
        "            results.append((sentence.strip(), predicted_emotion))\n",
        "    return results\n",
        "\n",
        "# Example text\n",
        "text = \"I am happy today. The weather is gloomy. why are you irritating me.\"\n",
        "\n",
        "# Predict emotions\n",
        "predicted_emotions = predict_emotions(text)\n",
        "\n",
        "# Highlight sentences with their emotions\n",
        "for sentence, emotion in predicted_emotions:\n",
        "    print(f\"{sentence}: {emotion}\")\n",
        "\n",
        "# Save the model's config and state_dict\n",
        "model_info = {\n",
        "    'config': model.config,\n",
        "    'state_dict': model.state_dict()\n",
        "}\n",
        "\n",
        "with open('saved_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model_info, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Define the mapping from label IDs to emotion names\n",
        "id2label = {\n",
        "    0: \"sad\",\n",
        "    1: \"happy\",\n",
        "    2: \"love\",\n",
        "    3: \"angry\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# Update the model's config with this mapping\n",
        "model.config.id2label = id2label\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_emotions(text):\n",
        "    sentences = text.split('.')\n",
        "    results = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip():  # Ignore empty sentences\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            predicted_class_id = logits.argmax().item()\n",
        "            predicted_emotion = model.config.id2label[predicted_class_id]\n",
        "            results.append((sentence.strip(), predicted_emotion))\n",
        "    return results\n",
        "\n",
        "# Example text\n",
        "text = \"Through turbulent skies and fleeting moments, a silent dance of shadows and whispers weaves a tapestry of human experience, embracing the ebb and flow of life's mysteries.\"\n",
        "\n",
        "# Predict emotions\n",
        "predicted_emotions = predict_emotions(text)\n",
        "\n",
        "# Highlight sentences with their emotions\n",
        "for sentence, emotion in predicted_emotions:\n",
        "    print(f\"{sentence}: {emotion}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfGfCxDqVL2S",
        "outputId": "f200d7c3-e5e6-4daf-f799-f4b481456367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Through turbulent skies and fleeting moments, a silent dance of shadows and whispers weaves a tapestry of human experience, embracing the ebb and flow of life's mysteries: sad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Specify the path to your .pkl file\n",
        "pkl_file_path = '/content/saved_model.pkl'\n",
        "\n",
        "# Load the model from the .pkl file\n",
        "with open(pkl_file_path, 'rb') as f:\n",
        "    loaded_model = pickle.load(f)\n",
        "\n",
        "# Check the type of the loaded object\n",
        "print(type(loaded_model))\n",
        "\n",
        "# If the loaded object is not the model itself, investigate the loading process\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixFc7hkUT4y0",
        "outputId": "a05bfa67-21ba-48a5-9eaf-e60af1522378"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the saved model info from the .pkl file\n",
        "with open('/content/saved_model.pkl', 'rb') as f:\n",
        "    model_info = pickle.load(f)\n",
        "\n",
        "# Initialize the model with the saved configuration\n",
        "model = AutoModelForSequenceClassification.from_config(model_info['config'])\n",
        "model.load_state_dict(model_info['state_dict'])\n",
        "\n",
        "# Define the tokenizer and model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Now you can use the loaded model for predictions\n"
      ],
      "metadata": {
        "id": "Wq4shoOCLIId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example text for prediction\n",
        "input_text = \" One of my friends also worked on turning on and off a particular lamp using a trigger word kind of as a fun project. But what I want to show you is how you can build a trigger word detection system. The literature on trigger detection algorithm is still evolving so there isn't wide consensus yet on what's the best algorithm for trigger word detection.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Perform inference using the loaded model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get the predicted class\n",
        "predicted_class_id = torch.argmax(outputs.logits[0]).item()\n",
        "\n",
        "# Map the predicted class ID to the corresponding emotion\n",
        "predicted_emotion = model.config.id2label[predicted_class_id]\n",
        "\n",
        "# Display the predicted emotion\n",
        "print(\"Predicted emotion:\", predicted_emotion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pMqQSk5NDzA",
        "outputId": "268d94df-9db6-4bfc-8737-601a89f7b518"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted emotion: happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the saved model info from the .pkl file\n",
        "with open('/content/saved_model.pkl', 'rb') as f:\n",
        "    model_info = pickle.load(f)\n",
        "\n",
        "# Initialize the model with the saved configuration\n",
        "model = AutoModelForSequenceClassification.from_config(model_info['config'])\n",
        "model.load_state_dict(model_info['state_dict'])\n",
        "\n",
        "# Define the tokenizer and model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define the mapping from label IDs to emotion names\n",
        "id2label = {\n",
        "    0: \"sad\",\n",
        "    1: \"happy\",\n",
        "    2: \"love\",\n",
        "    3: \"angry\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# Update the model's config with this mapping\n",
        "model.config.id2label = id2label\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_emotions(text):\n",
        "    sentences = text.split('.')\n",
        "    results = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip():  # Ignore empty sentences\n",
        "            inputs = tokenizer(sentence.strip(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            predicted_class_id = logits.argmax().item()\n",
        "            predicted_emotion = model.config.id2label[predicted_class_id]\n",
        "            results.append((sentence.strip(), predicted_emotion))\n",
        "    return results\n",
        "\n",
        "# Example text\n",
        "text = \"As the sun dipped below the horizon, casting a warm glow over the city, I couldn't help but feel a mix of emotions. Walking through the bustling streets, I noticed couples strolling hand in hand, their eyes filled with affection and love, which brought a sense of warmth to the cool evening air. However, amidst the laughter and chatter, a sudden loud argument erupted, sending ripples of tension and unease through the crowd. The flickering streetlights and looming shadows added a touch of fear, heightening the senses as night descended. Just then, a sudden burst of energy and excitement filled the sky as fireworks exploded in a dazzling display, leaving everyone in awe and surprise.\"\n",
        "\n",
        "# Predict emotions\n",
        "predicted_emotions = predict_emotions(text)\n",
        "\n",
        "# Highlight sentences with their emotions\n",
        "for sentence, emotion in predicted_emotions:\n",
        "    print(f\"{sentence}: {emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tok63fHJUyra",
        "outputId": "285a316e-bbf2-45db-831a-2c77b0c840a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As the sun dipped below the horizon, casting a warm glow over the city, I couldn't help but feel a mix of emotions: love\n",
            "Walking through the bustling streets, I noticed couples strolling hand in hand, their eyes filled with affection and love, which brought a sense of warmth to the cool evening air: love\n",
            "However, amidst the laughter and chatter, a sudden loud argument erupted, sending ripples of tension and unease through the crowd: angry\n",
            "The flickering streetlights and looming shadows added a touch of fear, heightening the senses as night descended: fear\n",
            "Just then, a sudden burst of energy and excitement filled the sky as fireworks exploded in a dazzling display, leaving everyone in awe and surprise: happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\n",
        "\"i feel happy\"]\n",
        "\n",
        "# Predict emotions\n",
        "predicted_emotions = predict_emotions(texts)\n",
        "\n",
        "# Highlight sentences with their emotions\n",
        "for sentence, emotion in predicted_emotions:\n",
        "    print(f\"{sentence}: {emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAnYNp8uYLC_",
        "outputId": "802bf0ac-f948-4195-9335-a9761d6ee744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i feel happy: surprise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Define the mapping from label IDs to emotion names\n",
        "id2label = {\n",
        "    0: \"sad\",\n",
        "    1: \"happy\",\n",
        "    2: \"love\",\n",
        "    3: \"angry\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "\n",
        "# Assuming 'model' and 'tokenizer' are already defined and trained\n",
        "\n",
        "# Update the model's config with this mapping\n",
        "model.config.id2label = id2label\n",
        "\n",
        "# Prepare the model information to save\n",
        "model_info = {\n",
        "    'config': model.config,\n",
        "    'state_dict': model.state_dict()\n",
        "}\n",
        "\n",
        "# Define the path to save the model in Google Drive\n",
        "model_path = '/content/drive/MyDrive/certificates/saved_model.pkl'\n",
        "\n",
        "# Save the model's config and state_dict to the specified path\n",
        "with open(model_path, 'wb') as f:\n",
        "    pickle.dump(model_info, f)\n",
        "\n",
        "print(f\"Model saved to {model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMRLyqhfY5z7",
        "outputId": "175440d0-4ecb-4ad0-ba9c-c7c2014209ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/certificates/saved_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r04n7eRcbIni",
        "outputId": "7ba4b0f9-62c7-4214-9203-0c396f709c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to the saved model in Google Drive\n",
        "model_path = '/content/drive/MyDrive/certificates/saved_model.pkl'\n",
        "\n",
        "# Load the saved model info from the .pkl file\n",
        "with open(model_path, 'rb') as f:\n",
        "    model_info = pickle.load(f)\n",
        "\n",
        "# Initialize the model with the saved configuration\n",
        "model = AutoModelForSequenceClassification.from_config(model_info['config'])\n",
        "model.load_state_dict(model_info['state_dict'])\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "# Define the tokenizer and model name\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Update the model's config with id2label mapping if not already present\n",
        "id2label = {\n",
        "    0: \"sad\",\n",
        "    1: \"happy\",\n",
        "    2: \"love\",\n",
        "    3: \"angry\",\n",
        "    4: \"fear\",\n",
        "    5: \"surprise\"\n",
        "}\n",
        "model.config.id2label = id2label\n",
        "\n",
        "# Define the prediction function\n",
        "def predict_emotions(text):\n",
        "    sentences = text.split('.')\n",
        "    results = []\n",
        "    for sentence in sentences:\n",
        "        if sentence.strip():  # Ignore empty sentences\n",
        "            inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                logits = outputs.logits\n",
        "                predicted_class_id = logits.argmax().item()\n",
        "                predicted_emotion = model.config.id2label[predicted_class_id]\n",
        "                results.append((sentence.strip(), predicted_emotion))\n",
        "    return results\n",
        "\n",
        "# Example text\n",
        "# Example text for prediction\n",
        "text = \"Under the gloomy skies, the family gathered to celebrate the graduation of their eldest son, who had worked tirelessly for years. Amidst the chatter and laughter, the grandmother's eyes sparkled with pride as she reminisced about the hard times they had endured. The father, feeling a surge of affection, gave his son a rare hug, which took the son by surprise but filled him with warmth. Meanwhile, in the corner, a heated argument erupted between two uncles over an old family dispute, their voices rising in anger, causing the younger children to watch with a mix of fear and curiosity. Suddenly, a loud crash from the kitchen startled everyone, only to reveal the cat had knocked over a vase, leaving the mother dismayed at the mess but relieved it was nothing serious. As the evening drew to a close, the family sat together, enjoying the calm after the storm, feeling the bittersweet blend of love, joy, and the underlying tensions that bound them together.\"\n",
        "\n",
        "# Predict emotions\n",
        "predicted_emotions = predict_emotions(text)\n",
        "\n",
        "# Highlight sentences with their emotions\n",
        "for sentence, emotion in predicted_emotions:\n",
        "    print(f\"{sentence}: {emotion}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENYkE_NgbT_s",
        "outputId": "0fcdc2cb-e269-44d3-e435-43f51af4c8b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Under the gloomy skies, the family gathered to celebrate the graduation of their eldest son, who had worked tirelessly for years: sad\n",
            "Amidst the chatter and laughter, the grandmother's eyes sparkled with pride as she reminisced about the hard times they had endured: happy\n",
            "The father, feeling a surge of affection, gave his son a rare hug, which took the son by surprise but filled him with warmth: love\n",
            "Meanwhile, in the corner, a heated argument erupted between two uncles over an old family dispute, their voices rising in anger, causing the younger children to watch with a mix of fear and curiosity: angry\n",
            "Suddenly, a loud crash from the kitchen startled everyone, only to reveal the cat had knocked over a vase, leaving the mother dismayed at the mess but relieved it was nothing serious: fear\n",
            "As the evening drew to a close, the family sat together, enjoying the calm after the storm, feeling the bittersweet blend of love, joy, and the underlying tensions that bound them together: love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fFbSfszeb7YR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}